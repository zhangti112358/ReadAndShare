感觉语言模型很适合用来做语音交互，做了一些尝试，这里记录开发过程和一些想法。
# 简单的语音交互程序
## demo
先看看结果，实现了自己常用的一些功能：
<video src="简单语音交互.mp4"></video>

## 实现功能和方式
实现的功能可以分为3类：
1. 操作系统和软件交互
2. 软件内部语音交互
3. 本地文件操作

功能上是这么分，但是从实现方式上看只有一种，每个对应的操作都设置一个文字描述，用语音去匹配文字。
实现的步骤是：
### 语音转换文字
使用语言模型， 这类模型有很多。
### 文字识别到操作
这是最重要的一步。最开始的想法是使用一个大语言模型搞定所有，试的过程中发现它的效果和鲁棒性不是很好（也许是prompt写的不够好。。），而且自己电脑跑不起来，调接口的延时也高到无法接受（几秒）。最终选择了用下面这几种混合起来：
+ 文字直接匹配
+ 拼音匹配
+ 句子匹配：用“Sentence Similarity”模型，计算句子相似性。

### 执行操作
就是windows上很常规的系统调用。比如start+程序名可以打开程序。程序内部不能语音交互，就用命令行参数完成一些支持的功能。比如“Start steam://rungameid/570”可以用steam打开id为570的游戏。

## 使用感受和想法
下面这些没什么特别的逻辑关系，想到什么就记下来了：
1. 语音，或者说文字的信息量很大。图形界面单个界面可以显示几个或者几十个选项，如果可能性很多的话就需要用嵌套多层或者用一个很长的列表。但是文字短短几个字就会有非常多的可能性。
2. 接上一条，语音交互有一种所想即所得的流畅感。因为不需要接收冗余信息，比如在多个图标里寻找。
3. 语音输入很好用但是听语音不是。现在更喜欢的方式是用语音输入，用眼睛阅读。不知道你用不用gpt或者豆包的语音输出功能，语音读一段长文本要很久，不如直接看一眼方便。
4. 几个字的匹配错误率高，同音字不好区分，反而句子长了之后准确率高，几乎不产生歧义。
5. 不同软件的语音交互是可以通用的。这次开发的程序里，软件层只写了各种操作的文字描述和调用接口，其他所有的包括模型，一些预计算，语音获取，识别，匹配都是共用的。

改善交互体验的想法：
1. 如果有端到端的模型更好。类似CLIP是图像生成特征向量，文字生成特征向量，然后两个向量计算相似度。语音和文字应该也可以。
2. 为了更低的延迟应该用实时转换。现在用的是语音输入完后转换，10s语音处理时间约1s，用起来不算卡但是没有话音刚落就执行的那种效果。
3. 现在只完成了语音的分类，是有限类的分类。对于任意的语音转换成操作，应该需要大语言模型，不过chat类的应用对输出的一致性和准确性没有那么敏感，对于高频使用的效率工具需要解决这些问题。

# 语音交互可能性
## demo
<video src="语音交互可能性.mp4"></video>

## 想法
人和计算机交互，从效率看如果把二进制数据和程序看作是1.0，命令行输入输出是2.0，图形用户界面是3.0，那结合语音交互的图形界面可以看作3.5。
其他想竞争3.x的应该还有手势交互，眼球交互。如果脑机接口实现了可以是4.0？
话说有定量分析（或者定性也行）交互方式效率的理论吗？
